{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "import seaborn as sns\n",
    "from data.cleaning import merge_datetime, get_dataset_from_pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City of Los Angeles: Proposing a Strategy for Optimizing Parking Enforcement Deployment\n",
    "**Author: Evan Gabrielson**\n",
    "\n",
    "### Overview: \n",
    "---\n",
    "In recent years, the City of Los Angeles has faced a significant financial challenge with its parking enforcement operations. Although traffic fines once provided a steady stream of revenue, a troubling shift occurred starting in 2017. Since then, the costs associated with salaries, equipment, and other expenses for parking enforcement have skyrocketed to over $809 million, while the revenue generated from parking ticket fines has lagged behind at $617 million. This $192 million shortfall highlights the urgent need for more efficient and effective strategies in managing parking violations and enforcement.\n",
    "\n",
    "### Business Understanding:\n",
    "---\n",
    "In his book titled \"The High Cost of Free Parking\", LA resident and UCLA professor Donald Shoup outlines the necessity of parking enforcement policy to \"curb\" the excess time and resources a free parking state begets. \n",
    "Shoup presents two key recommendations for reform to improve parking policy: pricing curb parking according to fair market value and redistributing parking revenue to neighborhoods for community investment.\n",
    "Parking enforcement is only necessary to the extent that it improves the maintenance and fair distribution of parking resources such that all actors can benefit equally from public road infrastructure.\n",
    "Today, however, LA Department of Transportation (LADOT) policy makers are forced to counteract skyhigh salary expenses with parking violation fees well above fair market value while reinvesting nothing back into the communities. \n",
    "Until LADOT can produce a net profit from parking enforcement, the citizenry of Los Angeles must continue to expect rising parking violation fees and zero community reinvestment.\n",
    "\n",
    "In this project, I propose several data-driven strategies for optimizing parking enforcer deployment which LADOT can employ to close the gap between revenue and payroll. \n",
    "Here are some facts about LADOT as it functions today:\n",
    "- LADOT currently deploys an equal distribution of enforcement officers across the City of LA. \n",
    "- Enforcers are on duty 24/7\n",
    "- Parking citations fees range from $53 - $350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data:\n",
    "---\n",
    "Parking citation data is available through the [City of LA's Data Catalog](https://data.lacity.org/Transportation/Parking-Citations/4f5p-udkv/about_data) and is updated on a daily basis. The following columns are of interest:\n",
    "- `issue_date`\n",
    "- `issue_time`\n",
    "- `fine_amount`\n",
    "- `agency`\n",
    "- `violation_code`\n",
    "- `loc_lat`\n",
    "- `loc_long`\n",
    "\n",
    "#### Data preparation and cleaning\n",
    "The `utils.py` script converts the dataset retrieved in JSON format from the Data Catalog API to a pandas DataFrame, ignoring unnecessary columns and invalid rows before storing the cleaned DataFrame to a pickle file. By \"invalid rows\" I refer to rows with missing values in critical columns such as `issue_date`, `issue_time` or `violation_code`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataset_from_pickle(\"data/pickle/citations_v0.pickle\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a DateTimeIndex\n",
    "\n",
    "We have two columns `issue_date` and `issue_time` that need to be combined in order to use pandas' built-in DateTimeIndex helper functions and cut down on memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new datetime column `issue_datetime` from the combined `issue_date` and `issue_time` columns\n",
    "df['issue_datetime'] = merge_datetime(df, 'issue_date', 'issue_time')\n",
    "\n",
    "# Drop unused columns\n",
    "df = df.drop(columns=['issue_date', 'issue_time'])\n",
    "\n",
    "# Set datetime index\n",
    "df = df.set_index('issue_datetime')\n",
    "\n",
    "# Only use violation data for 2014 through 2023\n",
    "df = df.loc[(df.index > '2013-12-31') & (df.index < '2024-1-1')]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting Datatypes\n",
    "\n",
    "Casting datatypes can help reduce the size of the DataFrame while minimizing the amount of error handling required later on in our analysis. The location of each citation is stored as longitude (`loc_long`) and latitude (`loc_lat`) values, which should be cast to floating-point columns. `fine_amount` should similarly be a floating-point column. We'll use `np.float32` to store these values as 32-bit floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast numerical columns to correct datatype\n",
    "df['loc_lat'] = df['loc_lat'].astype('float32')\n",
    "df['loc_long'] = df['loc_long'].astype('float32')\n",
    "df['fine_amount'] = df['fine_amount'].astype('float32')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two features remain as generic `object` type columns: `agency` and `violation_code`. Let's quickly identify if either of these are categorical columns by checking the number of unique values in each column using the `describe()` DataFrame member function. Categorical features are helpful for encoding string-type values into numerical values for machine learning algorithms and also for analyzing groups of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['agency', 'violation_code']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agency'] = df['agency'].astype('category')\n",
    "df['violation_code'] = df['violation_code'].astype('category')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing & Invalid Data\n",
    "\n",
    "There are several columns containing rows with null-ish values (i.e. NaN or 0.0). There are 65,080 rows with missing values which is likely due to data entry errors and optional fields on the citation form. For the sake of this analysis, we will ignore these rows which only occupy 0.3% of the total dataset and appear to be randomly distributed across the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20594138 entries, 2022-12-13 16:20:58.225000 to 2023-12-27 10:07:08.455000\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   fine_amount     float32 \n",
      " 1   agency          category\n",
      " 2   violation_code  category\n",
      " 3   loc_lat         float32 \n",
      " 4   loc_long        float32 \n",
      "dtypes: category(2), float32(3)\n",
      "memory usage: 451.7 MB\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset_from_pickle('data/pickle/citations_v0_bronze.pickle')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 65080 with invalid or null values.\n"
     ]
    }
   ],
   "source": [
    "# Isolate rows with columns containing NaN or 0.0 indicating an invalid value\n",
    "nullish_rows = (df['loc_lat'].isna() | (df['loc_lat'] == 0.0)) | (df['loc_long'].isna() | (df['loc_long'] == 0.0)) | (df['fine_amount'].isna() | (df['fine_amount'] == 0.0)) | (df['agency'].isna())\n",
    "print(f\"There are {nullish_rows.sum()} with invalid or null values.\")\n",
    "\n",
    "# Drop nullish rows\n",
    "df = df[nullish_rows == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fine_amount       0\n",
       "agency            0\n",
       "violation_code    0\n",
       "loc_lat           0\n",
       "loc_long          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc_lat        0\n",
       "loc_long       0\n",
       "fine_amount    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df[['loc_lat', 'loc_long', 'fine_amount']] == 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECKPOINT REACHED**\n",
    "\n",
    "We've now succesfully shrunk the DataFrame size by 65% from 1.3+ GB to 451 MB while standardizing the datetime, numerical, and categorical datatypes across all of the columns. Using the [Medallion data design pattern](https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion), we'll store progressively higher quality DataFrames into data stores labeled \"Bronze\", then \"Silver\", then \"Gold\". At this data processing checkpoint, we'll store the current DataFrame as a \"Bronze\"-tier pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fine_amount</th>\n",
       "      <th>agency</th>\n",
       "      <th>violation_code</th>\n",
       "      <th>loc_lat</th>\n",
       "      <th>loc_long</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-12-13 16:20:58.225</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4000A1</td>\n",
       "      <td>34.234493</td>\n",
       "      <td>-118.436005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-13 16:25:41.006</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4000A1</td>\n",
       "      <td>34.234493</td>\n",
       "      <td>-118.436005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-16 12:10:48.627</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4000A1</td>\n",
       "      <td>34.242790</td>\n",
       "      <td>-118.450142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19 08:45:06.121</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4000A1</td>\n",
       "      <td>34.243561</td>\n",
       "      <td>-118.450386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-19 08:55:35.605</th>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4000A1</td>\n",
       "      <td>34.243561</td>\n",
       "      <td>-118.450386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         fine_amount agency violation_code    loc_lat  \\\n",
       "issue_datetime                                                          \n",
       "2022-12-13 16:20:58.225         50.0      1         4000A1  34.234493   \n",
       "2022-12-13 16:25:41.006         50.0      1         4000A1  34.234493   \n",
       "2022-12-16 12:10:48.627         50.0      1         4000A1  34.242790   \n",
       "2022-12-19 08:45:06.121         50.0      1         4000A1  34.243561   \n",
       "2022-12-19 08:55:35.605         50.0      1         4000A1  34.243561   \n",
       "\n",
       "                           loc_long  \n",
       "issue_datetime                       \n",
       "2022-12-13 16:20:58.225 -118.436005  \n",
       "2022-12-13 16:25:41.006 -118.436005  \n",
       "2022-12-16 12:10:48.627 -118.450142  \n",
       "2022-12-19 08:45:06.121 -118.450386  \n",
       "2022-12-19 08:55:35.605 -118.450386  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_pickle('data/pickle/citations_v0_bronze.pickle')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to `01_exploratory_data_analysis.ipynb` to continue with the project walkthrough"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
