{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from data.cleaning import get_dataset_from_pickle\n",
    "from utils import adfuller_test, create_weekly_heatmap\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City of Los Angeles: Proposing a Strategy for Optimizing Parking Enforcement Deployment\n",
    "\n",
    "**Author: Evan Gabrielson**\n",
    "\n",
    "---\n",
    "\n",
    "A strategy for improving the efficiency of LADOT's Parking Enforcement division should function to save the city and citizenry of Los Angeles money. From public data published by the City of LA on their [Open Budget Explorer](https://openbudget.lacity.org/#!/year/2023/operating/0/department_name/Transportation/0/program_name/Parking+Enforcement+Services/0/source_fund_name), we can see that full and part-time salaries as well as overtime account for over 99% of allocated funding. While the official number of officers is not published publicly, we can use budget data and Indeed job postings by the City of LA to create a heuristic for the number of Full-Time Officers (FTO) working for the Parking Enforcement division.\n",
    "\n",
    "$$\\text{FTO Salary} = (\\$23.00 \\text{ per hour}) * (40 \\text{ hours per week}) * (52 \\text{ weeks per year}) =  \\$47,840 \\text{ annually}$$\n",
    "$$\\text{Number of FTOs} = \\$58,311,479 \\text{ budgeted for salaries } / \\text{ } \\$47,840 \\text{ per FTO salary} = 1,218 \\text{ FTOs}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fto_salary = 23 * 40 * 52\n",
    "num_ftos = 58.3e6 // fto_salary\n",
    "print(f\"Number of FTOs: {num_ftos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It appears that over **1218 full-time officer units** are working for the LADOT Parking Enforcement division. \n",
    "\n",
    "As previously stated, to optimize the City of LA's budget, profits must be maximized per officer deployed in a given region for a given shift. We can use historical parking citation data to understand the distribution of citations (1) spatially, across the various regions of Los Angeles, and (2) temporally, across the different days of the week. The citation density distribution is significant to the City of LA's budget optimization problem because it is __directly proportional to the maximum potential revenue that can be generated by the parking enforcement department__. Once a predictor for this distribution is known, we can better estimate the demand for enforcement officers by region and time to maximize revenue per officer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 437770 entries, 2013-12-31 07:00:00 to 2023-12-27 10:00:00\n",
      "Data columns (total 2 columns):\n",
      " #   Column          Non-Null Count   Dtype   \n",
      "---  ------          --------------   -----   \n",
      " 0   district        437770 non-null  category\n",
      " 1   citation_count  437770 non-null  int64   \n",
      "dtypes: category(1), int64(1)\n",
      "memory usage: 7.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Load timeseries\n",
    "citations_ts = get_dataset_from_pickle('data/pickle/timeseries.pickle')\n",
    "citations_ts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the simplicity of this initial study, we will focus on citation data only in the district of Hollywood. We will revisit the other districts in a final modeling stage to ensure district-to-district discrepancies are correctly incorporated in the models and to allow a complete view of the City of LA's potential budget savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "DatetimeIndex: 87555 entries, 2013-12-31 08:00:00 to 2023-12-27 10:00:00\n",
      "Series name: citation_count\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "87555 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract Hollywood-specific data\n",
    "hollywood_ts = citations_ts[citations_ts['district'] == 'Hollywood']['citation_count']\n",
    "hollywood_ts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at Hollywood's Mean Weekly Citation Density\n",
    "\n",
    "The most rudimentary approach to predicting citation density by hour and weekday is to look at an average of the citation density values across the entire timeseries for each combination of hour and weekday. It would be reasonable to assume that even this basic approach could represent a deployment scheme offering already considerable savings for the City of Los Angeles. Later on, we will examine models that take into account week-by-week differences in citation density to further enhance our savings and deployment demand prediction service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_weekly_heatmap(hollywood_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for stationarity\n",
    "\n",
    "Before timeseries modeling can begin, we'll use the Dickey-Fuller test for stationarity and isolate districts that are not stationary on a daily basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside:** Testing for non-stationarity with Dickey Fuller\n",
    "\n",
    "It is important for the sake of modeling and regression techniques that our timeseries is stationary. The Dickey Fuller test allows us to determine whether our timeseries is stationary by trying to establish a value for coefficient $\\rho$ in the following equation assuming our timeseries is non-stationary:\n",
    "\n",
    "$$\n",
    "x_t = \\alpha + \\rho x_{t-1} + \\epsilon_t \\quad \\text{where } \\alpha = \\text{drift \\& } \\epsilon_t = \\text{error term}\n",
    "$$\n",
    "\n",
    "By the definition of stationarity, we want to determine if the change in our timeseries ($\\Delta x_t$) has a tendency to return to some mean. This would only occur if $\\rho$ was less than 1.\n",
    "\n",
    "$$\n",
    "\\Delta x_t = (\\rho - 1) x_{t-1} + \\epsilon_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_0 : \\rho = 1 \\quad \\text{stationary}\\\\\n",
    "H_1 : \\rho < 1 \\quad \\text{non stationary}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print (Augmented) Dickey-Fuller test results\n",
    "adfuller_test(hollywood_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Model - ARIMA\n",
    "\n",
    "Let's split the training and test set and get a baseline model that predicts a constant citation count for all future timesteps. We'll use an ARIMA model with 0 autoregressive (AR) terms and 0 moving average (MA) filter terms to find the future citation count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 0.8\n",
    "split_index = int(hollywood_ts.shape[0] * TRAIN_SPLIT)\n",
    "train_ts, test_ts = hollywood_ts.iloc[:split_index], hollywood_ts.iloc[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_ts.index, train_ts.values\n",
    "X_test, y_test = test_ts.index, test_ts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = ARIMA(train_ts, order=(0, 0, 0), enforce_invertibility=False, enforce_stationarity=False)\n",
    "results = arima.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Akaike Information Criterion (AIC)](https://iowabiostat.github.io/research-highlights/joe/Cavanaugh_Neath_2019.pdf) value is an estimator of model quality, which when minimized results in the optimal model in terms of simplicity and accuracy. We can compare the AIC value of our Basic ARIMA Model (in this case: 902324) with the AIC value of more advanced modeling techniques to determine whether or not our more advanced models should be preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the trivial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RSME and ACF plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMA Model\n",
    "Capturing seasonality is important, we'll use a 24 hour period to try and pick up any daily seasonality. In order to isolate the best parameters, we'll use the `auto_arima` function to search across autoregressive (AR), differencing (I) and moving average (MA) terms, as well as seasonal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n",
    "                        FutureWarning)\n",
    "warnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n",
    "                        FutureWarning)\n",
    "\n",
    "# Define the p, d, q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, d, and q\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Basis represents number of periods in a \"season\" (7*24 = 168 weekly data points)\n",
    "basis = [7*24]\n",
    "seasonal_pdq = list(itertools.product(p, d, q, basis))\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Grid search for SARIMAX parameters\n",
    "for order in pdq:\n",
    "    for seasonal_order in seasonal_pdq:\n",
    "        try:\n",
    "            # SARIMAX Model\n",
    "            model = sm.tsa.SARIMAX(hollywood_ts, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "            # Fit model\n",
    "            model_fit = model.fit(disp=False, maxiter=200, method='lbfgs')\n",
    "            \n",
    "            # Store results in a dictionary\n",
    "            results.append({\n",
    "                'p': order[0],\n",
    "                'd': order[1],\n",
    "                'q': order[2],\n",
    "                'P': seasonal_order[0],\n",
    "                'D': seasonal_order[1],\n",
    "                'Q': seasonal_order[2],\n",
    "                's': basis,\n",
    "                'AIC': model_fit.aic\n",
    "            })\n",
    "            \n",
    "            print(f'SARIMAX{order} - AIC: {model_fit.aic}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'SARIMAX{order} failed with error: {e}')\n",
    "            continue\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Sort results by AIC value\n",
    "results_df = results_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sarima = SARIMAX(train_ts, order=(), seasonal_order=(), enforce_invertibility=False, enforce_stationarity=False)\n",
    "results = best_sarima.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SARIMA predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
