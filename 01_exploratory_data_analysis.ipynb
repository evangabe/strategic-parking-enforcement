{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-v0_8-notebook\")\n",
    "\n",
    "from utils import get_dataset_from_pickle\n",
    "\n",
    "import plotly.express as px\n",
    "px.set_mapbox_access_token(os.getenv('MAPBOX_TOKEN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City of Los Angeles: Proposing a Strategy for Optimizing Parking Enforcement Deployment\n",
    "**Author: Evan Gabrielson**\n",
    "\n",
    "### Overview: \n",
    "---\n",
    "In recent years, the City of Los Angeles has faced a significant financial challenge with its parking enforcement operations. Although traffic fines once provided a steady stream of revenue, a troubling shift occurred starting in 2017. Since then, the costs associated with salaries, equipment, and other expenses for parking enforcement have skyrocketed to over $809 million, while the revenue generated from parking ticket fines has lagged behind at $617 million. This $192 million shortfall highlights the urgent need for more efficient and effective strategies in managing parking violations and enforcement.\n",
    "\n",
    "### Business Understanding:\n",
    "---\n",
    "In his book titled \"The High Cost of Free Parking\", LA resident and UCLA professor Donald Shoup outlines the necessity of parking enforcement policy to \"curb\" the excess time and resources a free parking state begets. \n",
    "Shoup presents two key recommendations for reform to improve parking policy: pricing curb parking according to fair market value and redistributing parking revenue to neighborhoods for community investment.\n",
    "Parking enforcement is only necessary to the extent that it improves the maintenance and fair distribution of parking resources such that all actors can benefit equally from public road infrastructure.\n",
    "Today, however, LA Department of Transportation (LADOT) policy makers are forced to counteract skyhigh salary expenses with parking violation fees well above fair market value while reinvesting nothing back into the communities. \n",
    "Until LADOT can produce a net profit from parking enforcement, the citizenry of Los Angeles must continue to expect rising parking violation fees and zero community reinvestment.\n",
    "\n",
    "In this project, I propose several data-driven strategies for optimizing parking enforcer deployment which LADOT can employ to close the gap between revenue and payroll. \n",
    "Here are some facts about LADOT as it functions today:\n",
    "- LADOT currently deploys an equal distribution of enforcement officers across the City of LA. \n",
    "- Enforcers are on duty 24/7\n",
    "- Parking citations fees range from $53 - $350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20529058 entries, 2022-12-13 16:20:58.225000 to 2023-12-27 10:07:08.455000\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Dtype   \n",
      "---  ------          -----   \n",
      " 0   fine_amount     float32 \n",
      " 1   agency          category\n",
      " 2   violation_code  category\n",
      " 3   loc_lat         float32 \n",
      " 4   loc_long        float32 \n",
      "dtypes: category(2), float32(3)\n",
      "memory usage: 450.3 MB\n"
     ]
    }
   ],
   "source": [
    "citations_df = get_dataset_from_pickle('data/pickle/citations_v0_bronze.pickle')\n",
    "citations_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Outliers in Data\n",
    "Outlier analysis in exploratory data analysis is crucial because it helps identify data points that deviate significantly from the rest, which may indicate errors, unusual events, or important insights. Understanding these outliers allows for more accurate modeling, better decision-making, and can prevent misleading conclusions.\n",
    "\n",
    "When working with geospatial data, it is often a good idea to cluster coordinates using algorithmic approaches like KMeans and Agglomerative Clustering or, if available, overlay predefined regions that would be relevant to the model. While geospatial data may be useful for plotting, clusters are better for identifying patterns through machine learning and statistical analysis. A built-in perk of overlaying our geospatial data over the City of LA Parking Enforcement District map is that we automatically filter out data points that lie outside a valid district region. We'll use `geopandas` to facilitate the spatial join operation used to group rows by district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "district\n",
       "Hollywood    5025700\n",
       "Western      4595190\n",
       "Central      4467880\n",
       "Valley       3809492\n",
       "Southern     2556676\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreate citations DataFrame as GeoDataFrame\n",
    "citations_gdf = gpd.GeoDataFrame(citations_df, geometry=gpd.points_from_xy(citations_df['loc_long'], citations_df['loc_lat']))\n",
    "\n",
    "# Build districts GeoDataFrame and set Coordinate Reference System (CRS)\n",
    "districts_gdf = gpd.read_file(\"data/geojson/parking_enforcement_districts.geojson\")\n",
    "citations_gdf.set_crs(districts_gdf.crs, inplace=True)\n",
    "\n",
    "# Compute spatial join (sjoin) operation on 'District' GeoJSON attribute\n",
    "citations_joined_gdf = gpd.sjoin(citations_gdf, districts_gdf[['geometry', 'District']], how='left', predicate='within')\n",
    "\n",
    "# Store 'District' label in new 'district' column\n",
    "citations_df['district'] = citations_joined_gdf['District'].astype('category')\n",
    "citations_df['district'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the number of data points lying outside the predefined Parking Enforcement Districts and store a new pickle file with the `loc_lat` and `loc_long` columns then remove them to save memory space for future processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>loc_lat</th>\n",
       "      <th>loc_long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Central</td>\n",
       "      <td>34.057568</td>\n",
       "      <td>-118.240814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hollywood</td>\n",
       "      <td>34.084881</td>\n",
       "      <td>-118.320679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Southern</td>\n",
       "      <td>33.962658</td>\n",
       "      <td>-118.289276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Valley</td>\n",
       "      <td>34.191124</td>\n",
       "      <td>-118.454765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Western</td>\n",
       "      <td>34.029087</td>\n",
       "      <td>-118.424713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    district    loc_lat    loc_long\n",
       "0    Central  34.057568 -118.240814\n",
       "1  Hollywood  34.084881 -118.320679\n",
       "2   Southern  33.962658 -118.289276\n",
       "3     Valley  34.191124 -118.454765\n",
       "4    Western  34.029087 -118.424713"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find centroids for each district\n",
    "centroids = citations_df.groupby('district')[['loc_lat', 'loc_long']].mean().reset_index()\n",
    "centroids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(74120)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot geographic outliers\n",
    "\n",
    "for district in centroids:\n",
    "    plt.plot(district)\n",
    "citations_df['district'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Trends in Violations over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_df = pd.read_pickle('data/pickle/violations_v0_clean.pickle')\n",
    "violations_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_per_year = violations_df.resample('YE').count()\n",
    "print(violations_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of violations per year since 2000\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x=list(violations_per_year.index.year), y=violations_per_year['violation_code'])\n",
    "plt.title('Number of Violations per Year (2014-2023)', fontdict={'fontsize':22})\n",
    "plt.ylabel(\"Number of Violations (Millions)\",fontdict={'fontsize':16})\n",
    "plt.xlabel(\"Year\", fontdict={'fontsize':16})\n",
    "plt.savefig('./images/violations_per_year.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot seasonality decomposition\n",
    "timeseries = violations_df['violation_code'].resample('d').count().loc['2023-01-05':'2023-06-05']\n",
    "decomposition = seasonal_decompose(timeseries)\n",
    "trend, seasonal, residuals = decomposition.trend, decomposition.seasonal, decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(411)\n",
    "plt.title('Seasonality Decomposition for 2024')\n",
    "plt.plot(timeseries,label='Original',color='blue')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend,label='Trend',color='green')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonal',color='orange')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(414)\n",
    "plt.plot(residuals,label='Residuals',color='red')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Add plotly interactive map of violation count by region in representative date range (hexmap?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Analysis: Weekly & Hourly Basis\n",
    "TODO - Add findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate violations into weekdays and weekends\n",
    "weekdays = violations_df[violations_df.index.weekday<5]\n",
    "weekends = violations_df[violations_df.index.weekday>=5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "(weekdays.groupby(weekdays.index.hour).size()/10/52/5).plot(kind='area',label='Total Weekday Violations',linewidth=2,alpha=.5)\n",
    "(weekends.groupby(weekends.index.hour).size()/10/52/2).plot(kind = 'area',label='Total Weekend Violations',linewidth=2,alpha=.5)\n",
    "plt.legend()\n",
    "plt.title('Number of Violations per Hour',fontdict={'fontsize':22})\n",
    "plt.xlabel('Time Of Day (H)',fontdict={'fontsize':18})\n",
    "plt.ylabel('Violations per Week',fontdict={'fontsize':18})\n",
    "plt.savefig('./images/hm_weekday_vs_weekend.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Analysis: Daily Basis\n",
    "TODO - Add findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekday analysis\n",
    "weekdays_by_hour = violations_df.groupby([violations_df.index.hour,violations_df.index.weekday]).size()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(weekdays_by_hour.unstack(), xticklabels = ['Mon','Tues','Wed','Thurs','Fri','Sat','Sun'], linewidths=1)\n",
    "plt.title('Violations by Day of Week',fontdict={'fontsize':22})\n",
    "plt.ylabel('Hour',fontdict={'fontsize':16})\n",
    "plt.xlabel('Day of Week',fontdict={'fontsize':16})\n",
    "plt.savefig('./images/hm_daily.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial Analysis: Geographic Basis\n",
    "TODO - Add findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get violation counts grouped by latitude and longitude\n",
    "BIN_THRESHOLD = 0.02\n",
    "to_bin = lambda x: np.floor(x / BIN_THRESHOLD) * BIN_THRESHOLD\n",
    "violations_df['latbin'] = to_bin(violations_df['loc_lat'])\n",
    "violations_df['longbin'] = to_bin(violations_df['loc_long'])\n",
    "\n",
    "geographically_clustered_violations = violations_df.groupby([\n",
    "    'latbin', \n",
    "    'longbin'\n",
    "]).size().reset_index(name=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - [SECURITY RISK] Use .env token instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(geographically_clustered_violations, lat='latbin', lon='longbin', size=\"Count\", color=\"Count\", color_continuous_scale=px.colors.cyclical.IceFire)\n",
    "fig.update_layout(height=512, width=512)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Based on population density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Analysis: Violation Codes\n",
    "TODO - Add findings\n",
    "\n",
    "| Code | Description |\n",
    "| ---- | ----------- |\n",
    "| 80.69BS | PARKING PROHIBITED |\n",
    "| 88.13B+ | FAILURE TO PAY FOR A PARKING METER SPACE |\n",
    "| 80.56E4 | PARKED IN NO STOPPING CURB ZONE |\n",
    "| 80.58L | PARKED IN PREFERENTIAL PARKING DISTRICT WITHOUT PERMIT |\n",
    "| 5204A- | REGISTRATION TAB IMPROPERLY ATTACHED TO LICENSE |\n",
    "| 80.69B | PARKING PROHIBITED |\n",
    "| 5200 | LICENSE PLATE DISPLAY ISSUE |\n",
    "| 80.69C | TIME LIMIT EXCEEDED |\n",
    "| 80.69AP+ | CAR WITH TRAILER PARKING PROHIBITED | \n",
    "| 80.56 | PARKED IN PASSENGER LOADING ZONE |\n",
    "| 80.61 | PARKED IN ALLEY |\n",
    "| 22500E | BLOCKING DRIVEWAY |\n",
    "| 80.70 | PARKED IN ANTI-GRIDLOCK ZONE |\n",
    "| 80.69AA+ | PARKING PROHIBITED | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_THRESHOLD = violations_df.shape[0]*1//100\n",
    "print(f\"A given violation code must exceed a count of {INCLUDE_THRESHOLD} to be included in this analysis.\")\n",
    "violation_code_counts = violations_df['violation_code'].value_counts()\n",
    "filtered_codes = violation_code_counts[violation_code_counts > INCLUDE_THRESHOLD].index\n",
    "violation_code_counts = violations_df[violations_df['violation_code'].isin(filtered_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.countplot(data=violation_code_counts, y='violation_code', order=filtered_codes)\n",
    "plt.title('Number of Violations by Code (2014-2024)', fontdict={'fontsize':22})\n",
    "plt.ylabel(\"Violation Code\", fontdict={'fontsize':16})\n",
    "plt.xlabel(\"Number of Violations (Millions)\",fontdict={'fontsize':16})\n",
    "plt.savefig('./images/violations_by_code.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
